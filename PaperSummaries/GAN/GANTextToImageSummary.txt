Paper Summary for "Generative Adversarial Text to Image Synthesis" <2017-01-29 Sun>

* Paper from UMICH and Max Planck

* Single class image generation from text description

* Input is a single sentence human written description

* Outputs consist of multiple generated/halucinated images

* Problem can be summarized as:  first, learn a text feature representation that captures
the important visual details; and second, use these features to synthesize a compelling image that a human might
mistake for real.

* Zero shot learning case - generate something you have never seen before but have seen things of similar properties

* Uses Deep-Convolutional General Adversarial Networks (DC-GANs)

* Text encoding is done [[https://arxiv.org/pdf/1605.05395v1.pdf][Scott Reed's paper which does image summarization using CNN and LSTM]]

* Algorithm tested on CUB dataset of bird images, Oxford-201 dataset of flower images - outputs are pretty good

* Also tested a variant of the algortihm on MS-COCO dataset where the network "must" try to model a very complex scene with a lot of objects - `outputs are pretty crappy POTENTIAL FOR RESEARCH
